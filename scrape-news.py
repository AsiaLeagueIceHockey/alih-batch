import os
import feedparser
import datetime
from supabase import create_client, Client
from deep_translator import GoogleTranslator

# ğŸ’¡ ìƒˆë¡œ ì¶”ê°€: ì›¹í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ë° íŒŒì‹±
import requests
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright
from google import genai

# --- 0. RSS í”¼ë“œ ëª©ë¡ ---
# ì‚¬ìš©ìê°€ í™•ì •í•œ ë¦¬ìŠ¤íŠ¸
RSS_FEEDS = [
    {'url': 'https://news.google.com/rss/search?q=HL%EC%95%88%EC%96%91&hl=ko&gl=KR&ceid=KR:ko', 'language': 'ko'},
    {'url': 'https://news.google.com/rss/search?q=%EC%95%84%EC%9D%B4%EC%8A%A4%ED%95%98%ED%82%A4&hl=ko&gl=KR&ceid=KR:ko', 'language': 'ko'},
    {'url': 'https://news.google.com/rss/search?q=%EC%95%84%EC%8B%9C%EC%95%84%EB%A6%AC%EA%B7%B8+%EC%95%84%EC%9D%B4%EC%8A%A4%ED%95%98%ED%82%A4&hl=ko&gl=KR&ceid=KR:ko', 'language': 'ko'},
    {'url': 'https://news.google.com/rss/search?q=Asia+League+Ice+Hockey&hl=en-US&gl=US&ceid=US:en', 'language': 'en'},
    {'url': 'https://news.google.com/rss/search?q=%E3%82%A2%E3%82%B8%E3%82%A2%E3%83%AA%E3%83%BC%E3%82%B0%E3%82%A2%E3%82%A4%E3%82%B9%E3%83%9B%E3%83%83%E3%82%B1%E3%83%BC&hl=ja&gl=JP&ceid=JP:ja', 'language': 'ja'}
]

# --- 1. Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
def init_supabase():
    url = os.environ.get("SUPABASE_URL")
    key = os.environ.get("SUPABASE_SERVICE_KEY") 
    if not url or not key:
        print("Error: SUPABASE_URL or SUPABASE_SERVICE_KEY is not set.") 
        exit(1)
    return create_client(url, key)

# --- 2. DBì—ì„œ ê°€ì¥ ìµœì‹  ë‰´ìŠ¤ì˜ 'published_at' ê°€ì ¸ì˜¤ê¸° (ë™ì¼) ---
def get_latest_publish_time(supabase: Client) -> datetime.datetime:
    try:
        response = supabase.table('alih_news') \
                           .select('published_at') \
                           .order('published_at', desc=True) \
                           .limit(1) \
                           .execute()
        
        if response.data:
            latest_time_str = response.data[0]['published_at']
            return datetime.datetime.fromisoformat(latest_time_str)
        else:
            print("No existing data found. Setting baseline date to 2025-08-15.")
            return datetime.datetime.fromisoformat('2025-08-15T00:00:00+00:00')
            
    except Exception as e:
        print(f"Error fetching latest publish time: {e}")
        return datetime.datetime.fromisoformat('2025-08-15T00:00:00+00:00')

# --- 3. URL ì¶”ì¶œ í•¨ìˆ˜ (Playwright ì‚¬ìš©) ---
def get_final_url_sync(google_news_url: str) -> str:
    """
    Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ Google News URLì— ì ‘ê·¼í•˜ê³ , 
    JavaScript ê¸°ë°˜ ë¦¬ë‹¤ì´ë ‰íŠ¸ë¥¼ ë”°ë¼ ìµœì¢… ë„ì°© URLì„ ë™ê¸°ì ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ğŸš¨ ì£¼ì˜: GitHub Actions í™˜ê²½ì—ì„œëŠ” Playwright ì„¤ì¹˜ê°€ ì„ í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    try:
        with sync_playwright() as p:
            # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¡œ Chromium ì‹¤í–‰
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()

            # URLë¡œ ì´ë™ ë° ë„¤íŠ¸ì›Œí¬ í™œë™ì´ ì ì í•´ì§ˆ ë•Œê¹Œì§€ ëŒ€ê¸°
            page.goto(google_news_url, wait_until='networkidle', timeout=30000)
            
            final_url = page.url
            browser.close()
            
            # ë¦¬ë‹¤ì´ë ‰ì…˜ì´ ì‹¤íŒ¨í•˜ê³  Google News URLì´ ë‚¨ì•„ìˆë‹¤ë©´ None ë°˜í™˜
            if 'news.google.com/rss' in final_url:
                return google_news_url # ì‹¤íŒ¨ ì‹œ ì›ë˜ ë§í¬ ë°˜í™˜
                
            return final_url
            
    except Exception as e:
        print(f"Playwright URL ì¶”ì¶œ ì‹¤íŒ¨ for {google_news_url}: {e}")
        return google_news_url # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë˜ ë§í¬ ë°˜í™˜

# --- 4. ì›¹í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ ---
def extract_plain_text(url: str) -> str:
    """
    ì£¼ì–´ì§„ URLì—ì„œ HTMLì„ ê°€ì ¸ì™€ì„œ ë¶ˆí•„ìš”í•œ íƒœê·¸ë¥¼ ì œê±°í•˜ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        # User-Agent ì„¤ì • (curlì—ì„œ ì‚¬ìš©í–ˆë˜ ê²ƒê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •í•˜ì—¬ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ê²Œ í•©ë‹ˆë‹¤)
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
        }
        
        # requestsë¥¼ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. (ë¦¬ë‹¤ì´ë ‰íŠ¸ ìë™ ì²˜ë¦¬)
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status() # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
        
        # BeautifulSoupì„ ì‚¬ìš©í•˜ì—¬ HTML íŒŒì‹±
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ì£¼ì„ ë“± ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for script_or_style in soup(['script', 'style', 'noscript', 'meta', 'link', 'header', 'footer', 'nav', 'form']):
            script_or_style.decompose()
            
        # <body> íƒœê·¸ ë‚´ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ê³ , ì—¬ëŸ¬ ê°œì˜ ê³µë°±/ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
        text = soup.body.get_text(' ', strip=True)
        
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ë‚´ì„œ ë¡œê¹… ë¶€ë‹´ ì¤„ì´ê¸° (ì˜ˆ: ìµœëŒ€ 2000ì)
        MAX_TEXT_LENGTH = 2000
        if len(text) > MAX_TEXT_LENGTH:
             return text[:MAX_TEXT_LENGTH] + "..." # ì¼ë¶€ë§Œ ë°˜í™˜í•˜ê³  ì¤„ì„í‘œ ì¶”ê°€
        
        return text

    except requests.exceptions.RequestException as e:
        return f"[Error fetching content]: {e}"
    except Exception as e:
        return f"[Error parsing content]: {e}"

# --- 4.5 Gemini ìš”ì•½ í•¨ìˆ˜ ---
def get_ai_summary(text: str) -> str:
    """
    Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ 80ì ì´ë‚´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
    """
    # The client gets the API key from the environment variable `GEMINI_API_KEY`.
    # api_key ì²´í¬ëŠ” client ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ ì—†ì„ ì‹œ ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥í•˜ë¯€ë¡œ
    # ì•ˆì „í•˜ê²Œ í™˜ê²½ë³€ìˆ˜ í™•ì¸ í›„ ì§„í–‰
    if not os.environ.get("GEMINI_API_KEY"):
        print("Warning: GEMINI_API_KEY is not set. Skipping summary.")
        return ""
    
    try:
        client = genai.Client()
        
        # ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ ì‚¬ìš© (ë¹„ìš©/ì†ë„ ìµœì í™”)
        input_text = text[:4000] if len(text) > 4000 else text
        
        prompt = (
            "ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ í•œêµ­ì–´ë¡œ ì½ê¸° ì‰½ê²Œ 80ì ì´ë‚´ë¡œ ìš”ì•½í•´ì¤˜. "
            "ê¸°ê³„ì ì¸ ë²ˆì—­íˆ¬ë³´ë‹¤ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì´ë‚˜ ìš”ì•½ë¬¸ì²˜ëŸ¼ ì‘ì„±í•´ì¤˜. "
            "ê²°ê³¼ëŠ” ìš”ì•½ëœ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•´:\n\n"
            f"{input_text}"
        )
        
        response = client.models.generate_content(
            model="gemini-2.5-flash", contents=prompt
        )
        return response.text.strip()
    except Exception as e:
        print(f"Gemini summary failed: {e}")
        return ""

# --- 5. ë©”ì¸ íŒŒì‹± ë° ì €ì¥ ë¡œì§ (ìˆ˜ì •) ---
def main():
    supabase = init_supabase()
    translator = GoogleTranslator(source='auto', target='ko')
    latest_time = get_latest_publish_time(supabase)
    print(f"Fetching news published after: {latest_time}")

    articles_to_insert = []
    
    for feed in RSS_FEEDS:
        print(f"Parsing feed for language: {feed['language']}...")
        parsed_feed = feedparser.parse(feed['url'])
        
        for entry in parsed_feed.entries:
            try:
                entry_time_dt = datetime.datetime(*entry.published_parsed[:6], 
                                                tzinfo=datetime.timezone.utc)
                
                if entry_time_dt > latest_time:
                    created_at_dt = datetime.datetime.now(datetime.timezone.utc)
                    
                    original_title = entry.title
                    lang = feed['language']
                    
                    # ğŸ’¡ URL ì¶”ì¶œ: entry.link (Google News RSS URL)ì„ ì‚¬ìš©
                    # ----------------------------------------------------
                    google_link = entry.link
                    # 1. ìµœì¢… ì›ë³¸ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
                    origin_url = get_final_url_sync(google_link)
                    
                    if origin_url == google_link:
                        print(f"Warning: Failed to extract final URL for: {google_link}")
                    else:
                        print(f"Success: Extracted URL: {origin_url}")
                    # ----------------------------------------------------
                    
                    # ğŸ’¡ ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„: ì›ë³¸ URLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    # ----------------------------------------------------
                    article_content_text = extract_plain_text(origin_url)
                    
                    # ğŸ’¡ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¡œê¹…
                    print("\n--- Extracted Article Text (First 2000 chars) ---")
                    print(article_content_text)
                    print("---------------------------------------------------\n")
                    
                    # ----------------------------------------------------
                    
                    # ğŸ’¡ Geminië¥¼ ì´ìš©í•œ ìš”ì•½ (ë‚´ìš©ì´ ì¶©ë¶„íˆ ìˆì„ ë•Œë§Œ)
                    summary_text = ""
                    if article_content_text and len(article_content_text) > 100:
                        print("Summarizing article with Gemini...")
                        summary_text = get_ai_summary(article_content_text)
                        if summary_text:
                            print(f"Summary: {summary_text}")
                    
                    # ìš”ì•½ ì‹¤íŒ¨í•˜ê±°ë‚˜ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ì›ë³¸ ì œëª© ì‚¬ìš©
                    if not summary_text:
                        summary_text = original_title

                    translated_title = original_title
                    
                    # (ë²ˆì—­ ë¡œì§ ìƒëµ - ê¸°ì¡´ê³¼ ë™ì¼)
                    if lang != 'ko':
                        try:
                            # ... (ë²ˆì—­ ì½”ë“œ) ...
                            print(f"Translating from {lang}: {original_title}")
                            translated_title = translator.translate(original_title)
                        except Exception as e:
                            print(f"Translation failed for '{original_title}', using original: {e}")
                            translated_title = original_title
                    
                    row = {
                        'title': translated_title,
                        'summary': summary_text, # ğŸ’¡ ìˆ˜ì •: AI ìš”ì•½ë³¸ ì‚¬ìš©
                        'origin_url': origin_url,  # ğŸ’¡ ìˆ˜ì •: ì¶”ì¶œëœ ìµœì¢… URL ì‚¬ìš©
                        'language': lang, 
                        'published_at': entry_time_dt.isoformat(), 
                        'created_at': created_at_dt.isoformat()   
                    }
                    articles_to_insert.append(row)
                    
            except Exception as e:
                print(f"Error processing entry {entry.link}: {e}")

    # 5. í•„í„°ë§ëœ ìƒˆ ë‰´ìŠ¤ë§Œ DBì— ì‚½ì… (ë™ì¼)
    if articles_to_insert:
        print(f"Found {len(articles_to_insert)} new articles. Inserting to Supabase...")
        try:
            response = supabase.table('alih_news').upsert(
                articles_to_insert,
                on_conflict='origin_url',  
                ignore_duplicates=True  
            ).execute()
            
            print(f"Insert response: {response.data}")
            print(f"Successfully inserted/ignored {len(response.data)} articles.")
            
        except Exception as e:
            print(f"Error inserting data to Supabase: {e}")
    else:
        print("No new articles found.")

if __name__ == "__main__":
    main()